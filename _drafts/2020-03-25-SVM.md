---
layout: post
comments: true
title: "Support Vector Machine"
date: 2020-03-25
---

In this post, we will be looking at the classification technique known as Support Vector Machine (SVM). SVM is one of the most effective classification algorithm and is often found difficult to understand primarily because there are various kinds of concepts that are involved at play.

So far the algorithms that we have looked at (Linear and Logistic Regression) rely on Gradient Descent to achieve optimal values. However, SVM is slightly different. We will be using gradient here as well but not exactly in the same manner. 

### Pre-Requisites

There are certain prerequisites that one should know before understanding how SVMs work. Let's look at them - 

1. **Vectors and Planes** - 
- Equation of a plane P : ax + by + cz = k (3-dimensional)

- Normal Vector to the plane P (Vector perpendicular to the plane) : ai&#770; + bj&#770; + ck&#770;

- Distance of a point Q(x<sub>1</sub>,y<sub>1</sub>,z<sub>1</sub>) from the plane P : &plusmn; <sup> (ax<sub>1</sub> + by<sub>1</sub> + cz<sub>1</sub> -k) </sup> &frasl; <sub> &#8730;(a<sup>2</sup> + b<sup>2</sup> + c<sup>2</sup>) </sub>

The distance would be represented with a modulus but the value itself would be (+) or (-) depending in which side of the plane the point lies.

2. **Gradient** - 
- Gradient of a function is a vector points towards the maximum increase of the function

3. **General Fact** - 
- min<sub>x</sub> max<sub>&alpha;</sub> f(x,&alpha;) &ge; max<sub>&alpha;</sub> min<sub>x</sub> f(x,&alpha;)

4. **Langrangian Method** - This method is used to find optimal values of a function subject to certain constraints.

- **Initial Form**
Objective function : min<sub>x</sub> f(x) 

Equality Constraint : h(x) = 0

In order to solve this, we need to understand that the minima of f(x) will occur when both f(x) and h(x) are tangent to each other. The image below gives an illustration of the same. 

The image shows contours of f(x) = k. As we keep increasing the value of k, we see that the functions are tangent for the lowest k at which h(x) = 0

**insert image**

Since, both functions are tangent &Implies; **&nabla;<sub>x</sub> f = &lambda; (&nabla;<sub>x</sub> h)**

therefore, in order to solve this problem, we define a new Langragian function,

&#8466;(x,&lambda;) = f(x) + &lambda;(h(x))

Now, our objective is to just minimize this function. The conditions for this would be -

&nabla;<sub>x</sub> &#8466; = 0 and &nabla;<sub>&lambda;</sub> &#8466; = 0

&Implies; &nabla;<sub>x</sub> f = 0 and h(x) = 0 (This was our original condition as well).

The &lambda; here is called **langragian multiplier** and represents  <sup>&nabla; &#8466;</sup> &frasl; <sub>&nabla; h</sub> (The rate of increase of &#8466; as we relax the conditions on h)

- **Ext1 :** It has been shown that this can be extended for multiple constraints as well which is shown below - 

Objective function : min<sub>x</sub> f(x) 

Equality Constraint : h<sub>i</sub>(x) = 0 _where_ i = {1,2,3,...,n}

In this case, the Langrangian function would be -

&#8466;(x,&lambda;<sub>1</sub>,&lambda;<sub>2</sub>,...,&lambda;<sub>n</sub>) = f(x) + &Sigma;&lambda;<sub>i</sub> h<sub>i</sub>(x)

- **Ext2 :** Another extension of this is when we introduce inequality constraint instead of equality constraint into the picture.

Objective function : min<sub>x</sub> f(x) 

Inequality Constraint : g(x) &le; 0 

New objective function - &#8466;(x,&alpha;) = f(x) + &alpha;(g(x))

Here, If the x at which f(x) achieves minima satisfies the condition g(x) < 0 &Implies; the constraint is not required and &alpha; can be set to 0. 

If not, then again the minima will occur where both the functions are tangent to each other and so, g(x) = 0 (because the intersection point will lie on g(x))


### Notation
In Logistic Regression, we saw the notation used for success and failure were {0,1}. However, in SVM, the notation used is {-1, +1}.

